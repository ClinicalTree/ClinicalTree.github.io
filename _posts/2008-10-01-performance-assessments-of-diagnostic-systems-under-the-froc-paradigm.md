---
title: Performance Assessments of Diagnostic Systems Under the FROC Paradigm
author: [CL_AT_DavidGurScD,CL_AT_HowardERockettePhD]
date: 2008-10-01 00:00:00 +0700 +07
categories: [Academic Radiology, Volume 15, Issue 10]
tags: [Journals,General Radiology]
---
As use of free response receiver-operating characteristic (FROC) curves gains more acceptance for quantitatively assessing the performance of diagnostic systems, it is important that the experimentalist understands the possible role of this approach as one of the experimental design paradigms that are available to him or her among all other approaches as well as some of the issues associated with FROC type studies. In a number of experimental scenarios, the FROC paradigm and associated analytical tools have theoretical and practical advantages over both the binary and the ROC approaches to performance assessments of diagnostic systems, but it also has some limitations related to experimental design, data analyses, clinical relevance, and complexity in the interpretation of the results. These issues are rarely discussed and are the focus of this work.

In medicine—in general and radiology in particular—when an observer becomes an integral part of a diagnostic system, there are often difficulties assessing the performance of systems in an objective generalizable manner. We often become enamored by the development of new and more complex methodologies that reflect more subtle details of the evaluation performed by the human observers and for very good reasons. However, these studies are difficult to design and perform, are extremely costly, and may lead to an analysis that loses statistical power for actually clinically relevant questions because of the attempt to distinguish the effects of different aspects of the performance assessments. As important, generalizability of results depends, among other considerations, on the assumptions that are built into the study design. We should not forget that observer performance studies are designed in the hope of appropriately addressing a relevant clinical question in a manner that will withstand the test of time and hence enable important clinical practice decisions to be made. A specific question to be answered should lead to a less burdensome, practical (doable), and hopefully an optimal specific study design. The specific analytical tool to be used for analyses is but one of several available to the investigator in his or her “tool box”; the one selected should be the one most likely to provide reliable but clinically relevant conclusions ( ). Obviously, the experimentalist always desires to minimize the required sample size in terms of both readers and cases that are needed for inference generation; hence, statistical power is always an important consideration in selecting a specific study design ( ).

Recent developments in the field of observer performance experiments resulted in significant improvements of a very valuable tool: the free response receiver-operating characteristic (FROC) paradigm and the possibility of performing and analyzing studies under this approach. FROC methodology takes into consideration location of the suspected abnormality and allows for more than one location to be identified as suspicious ( ). As a result, the FROC approach enables detecting the differences within-subject (location-based) diagnostic performances, which are ignored in the subject-based ROC analysis ( ). Both parametric and nonparametric approaches can be used to analyze FROC data ( ). Some of the parametric approaches ( ) attempt to describe latent (unobserved) characteristics of a search process conducted specifically by human observers. As investigators learn more about the FROC approach and begin to understand it, we should remember that, as with all other approaches that yield tools for analyses of observer performance studies, FROC has both advantages and disadvantages that need to be understood. As others emphasize the advantages, and there are many, the experimentalist should be aware of some practical issues associated with the FROC approach and address these issues in his or her study design.

First, unlike ROC, in which an overall rating is provided for an image/case, under the FROC approach, observers are free to mark as many suspicious regions as they wish. Although some alternatives have been proposed ( ), it is a standard practice that to analyze the results of these studies an “acceptance target” has to be determined ( ). This target defines the distance from (or specific locations on) the center of the abnormalities in question that if marked inside this distance the observer gets the credit for “detecting” the suspicious region. Obviously, the size or shape of the acceptance target affect the results of the study, as described by Chakraborty et al ( ). We should remember that, in standard FROC studies, at one extreme when the full image represents an acceptance target, all marks on actually positive images (cases) are considered positive findings. At the other extreme, in which acceptance target is but one pixel (voxel) all marks (whether actually positive or not) are likely to be assigned as negative findings; hence, by definition the results will be affected by the acceptance target and the number of actual abnormalities present.

In many situations, the acceptance target of different abnormalities could overlap and one has to a priori decide how to address the marks that belong to the intersection. Some of the possible approaches can involve setting smaller targets or having a rule-based approach to address this issue. This may become a significant problem when an easily detectable benign finding (with relatively low importance) is located near a subtle malignant finding (with high importance). Incorrect handling of the marks that “hit” both lesions could have a significant impact on the actual relevance of the results of these studies. Investigators should be aware of this and other similar issues when performing FROC studies with multiple abnormalities, some of which are in close proximity.

Correct handling and assignments of ratings becomes an even more difficult issue when more than one image per examination is provided to the observer (eg, two-view mammography or two-view chest) and “paired” markings are ascertained between the two presentations of the same abnormality (on each of the images). When analyzing a “case-based” (eg, breast-based) performance using both ratings of an abnormality by averaging the two ratings, or using the maximum rating between the two, correct assignments of the ratings to the appropriate abnormality is of utmost importance. The effect of “acceptance target” aggravates the problem even further. Therefore, decision rules regarding how these situations are considered and accounted for should be addressed (and stated) a priori.

[Get Radiology Tree app to read full this article<](https://clinicalpub.com/app)

[Get Radiology Tree app to read full this article<](https://clinicalpub.com/app)

[Get Radiology Tree app to read full this article<](https://clinicalpub.com/app)

[Get Radiology Tree app to read full this article<](https://clinicalpub.com/app)

[Get Radiology Tree app to read full this article<](https://clinicalpub.com/app)

[Get Radiology Tree app to read full this article<](https://clinicalpub.com/app)

[Get Radiology Tree app to read full this article<](https://clinicalpub.com/app)

[Get Radiology Tree app to read full this article<](https://clinicalpub.com/app)

## References

- 1\. Wagner R.F., Metz C.E., Campbell G.: Assessment of medical imaging systems and computer aids: a tutorial review \[review\]. Acad Radiol 2007; 14: pp. 723-748.


- 2\. Krupinski E.A., Jiang Y.: Anniversary paper: evaluation of medical imaging systems. Med Phys 2008; 35: pp. 645-659.


- 3\. Hillis S.L., Berbaum K.S.: Power estimation for the Dorfman-Berbaum-Metz method. Acad Radiol 2004; 11: pp. 1260-1273.


- 4\. Obuchowski N.A.: Multireader receiver operating characteristic studies: a comparison of study designs. Acad Radiol 1995; 2: pp. 709-716.


- 5\. Chakraborty D.P., Berbaum K.S.: Observer studies involving detection and localization: modeling, analysis, and validation. Med Phys 2004; 31: pp. 2313-2330.


- 6\. Chakraborty D.P.: A search model and figure of merit for observer data acquired according to the free-response paradigm. Phys Med Biol 2006; 51: pp. 3449-3462.


- 7\. Zheng B., Chakraborty D.P., Rockette H.E., et. al.: A comparison of two data analyses from two observer performance studies using Jackknife ROC and JAFROC. Med Phys 2005; 32: pp. 1031-1034.


- 8\. Edwards D.C., Kupinski M.A., Metz C.E., et. al.: Maximum likelihood fitting of FROC curves under an initial-detection-and-candidate-analysis model. Med Phys 2002; 29: pp. 2861-2870.


- 9\. Yoon H.J., Zheng B., Sahiner B., et. al.: Evaluating computer-aided detection algorithms. Med Phys 2007; 34: pp. 2024-2038.


- 10\. Bandos A.L., Rockette H.E., Song T., et. al.: Area under the free-response ROC curve (FROC) and a related summary index. Biometrics 2008; epub ahead of print


- 11\.  Samuelson FW, Petrick N. Comparing image detection algorithms using resampling. Biomedical imaging: macro to nano, 3rd IEEE International Symposium: Arlington, VA: April 6–9, 2006, 1312–1315.


- 12\. Chakraborty D., Yoon H.J., Mello-Thoms C.: Spatial localization accuracy of radiologists in free-response studies: Inferring perceptual FROC curves from mark-rating data. Acad Radiol 2007; 14: pp. 4-18.


- 13\. Chakraborty D.P.: ROC curves predicted by a model of visual search. Phys Med Biol 2006; 51: pp. 3463-3482.


- 14\. Song T., Bandos A.I., Rockette H.E., Gur D.: On comparing methods for discriminating between actually negative and actually positive subjects with FROC type data. Med Phys 2008; 35: pp. 1547-1558.


- 15\. van Engeland S., Karssemeijer N.: Combining two mammographic projections in a computer aided mass detection method. Med Phys 2007; 34: pp. 898-905.


- 16\. Wei J., Chan H.P., Sahiner B., et. al.: Dual system approach to computer-aided detection of breast masses on mammograms. Med Phys 2006; 33: pp. 4157-4168.


- 17\. Wagner R.F., Beam C.A., Beiden S.V.: Reader variability in mammography and its implications for expected utility over the population of readers and cases. Med Decis Making 2004; 24: pp. 561-572.


- 18\.  Gur D, Bandos AI, Cohen CS, et al. The “laboratory” effect: comparing radiologists' performance and variability during clinical prospective and laboratory mammography interpretations. Radiology. In press.