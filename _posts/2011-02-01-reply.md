---
title: Reply
author: [David Gur ScD,Andriy I. Bos PhD,Howard E. Rockette PhD]
date: 2011-02-01 00:00:00 +0700 +07
categories: [{Academic Radiology, Volume 18, Issue 2 SOURCE CL_S_AcademicRadiologyVolume18Issue2 1}]
tags: [Journals,General Radiology]
---
We thank the authors for their interest in our work and we appreciate the opportunity to clarify several issues related to the use of binary versus multicategory rating approaches in observer performance studies and related data analyses. We emphasize that the issues discussed in our original article relate primarily to observer performance studies where some of the properties inherent to completely automated (computerized) systems are either violated or in question.

We first highlight several facts about our article that could potentially be misinterpreted as a result of the comments made by Samuelson et al. The main objective of our article was to advise researchers conducting an observer performance study not to discard the binary paradigm altogether solely because of the potentially higher statistical power of the multicategory receiver operating characteristic (ROC) approach. We did not claim that the binary paradigm is better than the ROC approach under all (or even most) circumstances. Our article acknowledges that, in many instances, the conventional ROC approach is indeed a better method. However, we highlighted the fact that there are circumstances when the characteristics of binary decisions could and should be considered. Furthermore, we neither stated nor implied that the area under the curve (AUC) associated with a single point is an approximation of the area under the entire ROC curve. On the contrary, in this article, as in others , we repeatedly emphasized that these are two theoretically different, albeit related and conceptually similar, summary indices that could lead not only to different but at times to actually contradictory conclusions.

In their first point, Samuelson et al argue that an ROC curve corresponding to a multicategory rating experiment describes the characteristics of all binary operating modes achievable by the system, including the performance characteristics in a binary task (eg, recall/no recall). Although this is by definition true for completely automated computerized systems (eg, CAD alone) where binary decisions are driven by a specific dichotomization of ratings, this is merely a conjecture for diagnostic procedures that include the human observer as an integral part of a system. Behavior and performance characteristics of human observers can be influenced by a large number of factors, including the rating scale being employed. The possible effect of the rating format in various observer performance studies has been acknowledged by researchers previously . Although in some studies the effect of the rating scale was found to be negligible for specific (targeted) analytical inferences, the possible impact of the rating scale in other tasks cannot be discarded. The potential difference in behavior and thereby the resulting performance of observers when using a binary scale (which is an extreme case of a categorical scale), rather than a multicategory or a quasi-continuous scale, could be profound.

Furthermore, although in many instances and for many tasks the multicategory rating format can be advantageous , in some diagnostic tasks it could be suboptimal . In some diagnostic tasks (eg, breast imaging based diagnostic workup rated using a diagnostic BIRADs scale), a multicategory rating scale is actually used in the clinic. In other diagnostic tasks (eg, detection of interstitial disease), although a binary scale (present/not present) is typically used in the clinic, radiologists are likely to be able to consistently express their confidence level in the presence of disease using a multicategory scale . Yet, there are well-defined diagnostic tasks (eg, detection of microcalcification on mammograms, pneumothoraces, or rib fractions on posteroanterior chest images) where the opinions of radiologists are essentially intrinsically binary . When performing these diagnostic tasks, the operating characteristics resulting from binary decisions may be different from any of the points on the ROC curve that results from the use of a multicategory rating experiment, even when the same subjects are considered (implied) in both experiments.

As correctly noted by Samuelson et al in their second point, the operating characteristics of human observers in a binary task vary substantially among observers . However, this is not a sufficient reason for discarding the use of binary scales. Rather, it constitutes but one of the reasons for the possible importance of the binary scale as a potential source of clinically relevant information. It may even be more so because these characteristics are difficult to manipulate. The specific inherent and actual operating modes that are “natural” to the radiologists cannot always be derived from a conventional multicategory ROC type study.

Any summary measure estimated in the laboratory, under either a binary or a multicategory rating scale, could be biased with respect to the operating characteristics that are actually achieved in the clinic . Possible reasons for biases, a fraction of which is mentioned by Samuelson et al, result in a potential for differences between observer performance levels measured in experimental and clinical environments. To reduce the potential effect of possible biases, researchers designing observer performance studies are frequently advised to address a broad spectrum of issues, including formulating adequate and specific instructions to observers and maintaining conditions as similar as possible to the clinical environment . Discrepancy between internal perception scales used by radiologists in the clinic and rating scales used in observer performance experiments could lead to biases in generalization from the laboratory to the clinic, and there are instances where the use of the binary scale could lead to a smaller bias in this respect . In tasks where either of the scales could be used, the relative magnitude of the possible bias of the study conclusions reached under a binary and/or a multicategory scale is generally unknown and it could depend on a number of parameters specific to the observer performance study in question.

[Get Radiology Tree app to read full this article<](https://clinicalpub.com/app)

[Get Radiology Tree app to read full this article<](https://clinicalpub.com/app)

[Get Radiology Tree app to read full this article<](https://clinicalpub.com/app)

[Get Radiology Tree app to read full this article<](https://clinicalpub.com/app)

[Get Radiology Tree app to read full this article<](https://clinicalpub.com/app)

## Acknowledgments

[Get Radiology Tree app to read full this article<](https://clinicalpub.com/app)

## References

- 1\. Gur D., Bandos A.I., Rockette H.E., et. al.: Is an ROC-type response truly always better than a binary response in observer performance studies?. Acad Radiol 2010; 17: pp. 639-645.


- 2\. Gur D., Bandos A.I., Klym A.H., et. al.: Agreement of the order of overall performance levels under different reading paradigms. Acad Radiol 2008; 15: pp. 1567-1573.


- 3\. Swets J.A., Picket R.M.: Evaluation of diagnostic systems: methods from signal detection theory.1982.Academic PressNew York


- 4\. Zhou X.H., Obuchowski N.A., McClish D.K.: Statistical methods in diagnostic medicine.2002.Wiley & Sons IncNew York


- 5\. Rockette H.E., Gur D., Cooperstein L.A., et. al.: Effect of two rating formats in multi-disease ROC study of chest images. Invest Radiol 1990; 25: pp. 225-229.


- 6\. Wagner R.F., Beiden S.V., Metz C.E.: Continuous versus categorical data for ROC analysis: some quantitative considerations. Acad Radiol 2001; 8: pp. 328-334.


- 7\. Gur D., Rockette H.E., Bandos A.I.: “Binary” and “non-binary” detection tasks: are current performance measures optimal?. Acad Radiol 2007; 14: pp. 871-876.


- 8\. Herron J.M., Bender T., Campbell W.L., et. al.: Effects of luminance and resolution on observer performance with chest radiographs. Radiology 2000; 215: pp. 169-174.


- 9\. Beam C.A., Layde P.M., Sullivan D.C.: Variability in the interpretation of screening mammograms by US radiologists: findings from a national sample. Arch Intern Med 1996; 156: pp. 209-213.


- 10\. Gur D., Bandos A.I., Cohen C.S., et. al.: The “laboratory” effect: comparing radiologists’ performance and variability during clinical prospective and laboratory mammography interpretations. Radiology 2008; 247: pp. 12-15.


- 11\. Straub W.H., Gur D., Good W.F., et. al.: Primary CT diagnosis of abdominal masses in a PACS environment. Radiology 1991; 178: pp. 739-743.


- 12\. Slasky B.S., Gur D., Good W.F., et. al.: Receiver operating characteristic analysis of chest image interpretation with conventional, laser printed, and high-resolution workstation images. Radiology 1990; 174: pp. 775-780.


- 13\. Gur D., Rockette H.E., Armfield D.R., et. al.: The prevalence effect in a laboratory environment. Radiology 2003; 228: pp. 10-14.


- 14\. Gur D., Good W.F., Oliver J.H., et. al.: Sequential viewing of abdominal CT images at varying rates. Radiology 1994; 191: pp. 119-122.